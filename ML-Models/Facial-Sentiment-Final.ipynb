{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c663d800-4079-40ae-81fa-e14fbbde20e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ashut\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "\n",
    "# Create our custom emotion detection model\n",
    "def create_custom_model(input_shape=(48, 48, 1)):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First Convolutional Block\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # Second Convolutional Block\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # Flatten and Dense Layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))  # 10 emotions: 7 basic + 3 custom\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize the face cascade\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Extended emotion labels including our custom states\n",
    "emotion_labels = {\n",
    "    0: 'Angry',\n",
    "    1: 'Disgust',\n",
    "    2: 'Fear',\n",
    "    3: 'Happy',\n",
    "    4: 'Sad',\n",
    "    5: 'Surprise',\n",
    "    6: 'Neutral',\n",
    "    7: 'Excited',\n",
    "    8: 'Confident',\n",
    "    9: 'Nervous',\n",
    "    10: 'Confused',\n",
    "    11: 'Blanked-out'\n",
    "}\n",
    "\n",
    "class EmotionAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.prev_emotions = []\n",
    "        self.emotion_history = []\n",
    "        self.face_time = 0\n",
    "        self.no_face_time = 0\n",
    "        self.last_major_movement = time.time()\n",
    "        self.prev_face_position = None\n",
    "        self.movement_threshold = 30  # pixels\n",
    "        self.blink_count = 0\n",
    "        self.last_blink_time = time.time()\n",
    "        self.eye_aspect_ratio_threshold = 0.3\n",
    "        \n",
    "    def detect_custom_states(self, face_img, emotion_probs, face_position):\n",
    "        # Get the base emotion and probabilities\n",
    "        emotions = {}\n",
    "        for emotion, score in emotion_probs.items():\n",
    "            emotions[emotion] = score\n",
    "            \n",
    "        # Sort emotions by probability\n",
    "        sorted_emotions = sorted(emotions.items(), key=lambda x: x[1], reverse=True)\n",
    "        base_emotion = sorted_emotions[0][0]\n",
    "        \n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Add the current base emotion to our history\n",
    "        self.emotion_history.append(base_emotion)\n",
    "        if len(self.emotion_history) > 30:  # Keep last 30 frames (about 1 second)\n",
    "            self.emotion_history.pop(0)\n",
    "        \n",
    "        # Calculate movement\n",
    "        movement = 0\n",
    "        if self.prev_face_position is not None:\n",
    "            movement = np.sqrt((face_position[0] - self.prev_face_position[0])**2 + \n",
    "                              (face_position[1] - self.prev_face_position[1])**2)\n",
    "            \n",
    "            if movement > self.movement_threshold:\n",
    "                self.last_major_movement = current_time\n",
    "        \n",
    "        self.prev_face_position = face_position\n",
    "        \n",
    "        # Time since last major movement\n",
    "        time_since_movement = current_time - self.last_major_movement\n",
    "        \n",
    "        # Analyze emotion patterns\n",
    "        emotion_counts = {}\n",
    "        for emotion in self.emotion_history:\n",
    "            if emotion in emotion_counts:\n",
    "                emotion_counts[emotion] += 1\n",
    "            else:\n",
    "                emotion_counts[emotion] = 1\n",
    "        \n",
    "        dominant_emotion = max(emotion_counts, key=emotion_counts.get) if emotion_counts else None\n",
    "        emotion_stability = emotion_counts.get(dominant_emotion, 0) / len(self.emotion_history) if self.emotion_history else 0\n",
    "        \n",
    "        # Detect custom states based on heuristics\n",
    "        custom_emotion = base_emotion\n",
    "        custom_prob = emotions[base_emotion]\n",
    "        \n",
    "        if base_emotion == 'happy' and movement > self.movement_threshold * 1.5:\n",
    "            custom_emotion = 'Excited'\n",
    "            custom_prob = emotions[base_emotion] * 0.9  # Slightly reduce confidence for derived state\n",
    "        \n",
    "        elif emotion_stability > 0.7 and base_emotion in ['neutral', 'happy'] and time_since_movement > 2:\n",
    "            custom_emotion = 'Confident'\n",
    "            custom_prob = emotions[base_emotion] * 0.85\n",
    "        \n",
    "        elif base_emotion in ['fear', 'surprise'] and movement > self.movement_threshold:\n",
    "            custom_emotion = 'Nervous'\n",
    "            custom_prob = emotions[base_emotion] * 0.9\n",
    "        \n",
    "        elif len(set(self.emotion_history[-10:])) >= 4:  # Rapidly changing emotions\n",
    "            custom_emotion = 'Confused'\n",
    "            custom_prob = 0.7  # Assign a reasonable confidence\n",
    "        \n",
    "        elif time_since_movement > 5 and base_emotion == 'neutral':\n",
    "            custom_emotion = 'Blanked-out'\n",
    "            custom_prob = 0.75\n",
    "        \n",
    "        # Create a new dictionary with all emotions including our custom ones\n",
    "        result = dict(emotions)\n",
    "        \n",
    "        # Only add the custom emotion if it's different from the base emotion\n",
    "        if custom_emotion != base_emotion:\n",
    "            result[custom_emotion] = custom_prob\n",
    "            \n",
    "        # Sort all emotions by probability\n",
    "        sorted_result = sorted(result.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return sorted_result\n",
    "\n",
    "def main():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    analyzer = EmotionAnalyzer()\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Mirror the image horizontally\n",
    "        frame = cv2.flip(frame, 1)\n",
    "            \n",
    "        # Convert to grayscale for face detection\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detect faces\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        \n",
    "        for (x, y, w, h) in faces:\n",
    "            # Extract face ROI\n",
    "            face_roi = frame[y:y+h, x:x+w]\n",
    "            \n",
    "            try:\n",
    "                # Use DeepFace for base emotion detection\n",
    "                analysis = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
    "                \n",
    "                # Get the emotion probabilities\n",
    "                if isinstance(analysis, list):\n",
    "                    emotion_probs = analysis[0]['emotion']\n",
    "                else:\n",
    "                    emotion_probs = analysis['emotion']\n",
    "                \n",
    "                # Detect custom emotional states\n",
    "                face_position = (x + w/2, y + h/2)  # Center of the face\n",
    "                custom_emotions = analyzer.detect_custom_states(face_roi, emotion_probs, face_position)\n",
    "                \n",
    "                # Draw rectangle around face\n",
    "                color = (0, 255, 0)  # Green\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "                \n",
    "                # Display top emotions\n",
    "                if len(custom_emotions) >= 2:\n",
    "                    top_emotion = custom_emotions[0]\n",
    "                    second_emotion = custom_emotions[1]\n",
    "                    \n",
    "                    # Only show both if second emotion has probability > 37%\n",
    "                    if second_emotion[1] > 37:\n",
    "                        emotion_text = f\"{top_emotion[0]}: {top_emotion[1]:.1f}%, {second_emotion[0]}: {second_emotion[1]:.1f}%\"\n",
    "                    else:\n",
    "                        emotion_text = f\"{top_emotion[0]}: {top_emotion[1]:.1f}%\"\n",
    "                else:\n",
    "                    emotion_text = f\"{custom_emotions[0][0]}: {custom_emotions[0][1]:.1f}%\"\n",
    "                \n",
    "                cv2.putText(frame, emotion_text, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in emotion detection: {e}\")\n",
    "                # Just draw the face rectangle if emotion detection fails\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "        \n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Advanced Emotion Detection', frame)\n",
    "        \n",
    "        # Break the loop on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95d7700-6c93-483f-8e6a-30eaf0ca64ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
