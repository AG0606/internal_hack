{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8fde001-7ccc-452a-825f-473bea1a76c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained model not found. In a real application, you would download it here.\n",
      "For this demo, you'll need to train your own model or provide a pre-trained one.\n",
      "No model found. You'll need to provide a pre-trained model.\n",
      "For demonstration purposes, a dummy model would be created here.\n",
      "New model created. You need to train it before use.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashut\\AppData\\Local\\Temp\\ipykernel_17124\\1117512882.py:458: UserWarning: frames=None which we can infer the length of, did not pass an explicit *save_count* and passed cache_frame_data=True.  To avoid a possibly unbounded cache, frame data caching has been disabled. To suppress this warning either pass `cache_frame_data=False` or `save_count=MAX_FRAMES`.\n",
      "  self.ani = animation.FuncAnimation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing set to: 0.3\n",
      "Threshold set to: 0.01\n",
      "Sensitivity set to: 1.2\n",
      "Recording started\n",
      "Detected emotion: angry - 0.00\n",
      "Detected emotion: angry - 0.00\n",
      "Detected emotion: angry - 0.00\n",
      "Detected emotion: angry - 0.00\n",
      "Detected emotion: angry - 0.00\n",
      "Detected emotion: angry - 0.00\n",
      "Detected emotion: angry - 0.00\n",
      "Detected emotion: angry - 0.00\n",
      "Detected emotion: angry - 0.00\n",
      "Detected emotion: angry - 0.00\n",
      "Detected emotion: angry - 0.00\n",
      "Prediction error: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "Detected emotion: neutral - 0.00\n",
      "Prediction error: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "Detected emotion: neutral - 0.00\n",
      "Prediction error: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "Detected emotion: neutral - 0.00\n",
      "Prediction error: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "Detected emotion: neutral - 0.00\n",
      "Prediction error: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "Detected emotion: neutral - 0.00\n",
      "Prediction error: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "Detected emotion: neutral - 0.00\n",
      "Detected emotion: angry - 0.00\n",
      "Threshold set to: 0.009\n",
      "Threshold set to: 0.008\n",
      "Threshold set to: 0.007\n",
      "Threshold set to: 0.006\n",
      "Detected emotion: angry - 0.00\n",
      "Detected emotion: angry - 0.00\n",
      "Detected emotion: angry - 0.00\n",
      "Threshold set to: 0.004\n",
      "Threshold set to: 0.003\n",
      "Detected emotion: angry - 0.00\n",
      "Threshold set to: 0.002\n",
      "Detected emotion: angry - 0.00\n",
      "Detected emotion: angry - 0.00\n",
      "Recording stopped\n",
      "Resources released\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import joblib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pyaudio\n",
    "import wave\n",
    "import threading\n",
    "import time\n",
    "import queue\n",
    "from datetime import datetime\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from tkinter import Scale, HORIZONTAL\n",
    "\n",
    "class RealtimeSpeechEmotionRecognizer:\n",
    "    def __init__(self, model_path=None, scaler_path=None):\n",
    "        \"\"\"\n",
    "        Initialize the Real-time Speech Emotion Recognition model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_path : str, optional\n",
    "            Path to a saved model file to load (default: None)\n",
    "        scaler_path : str, optional\n",
    "            Path to a saved scaler file to load (default: None)\n",
    "        \"\"\"\n",
    "        self.emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "        self.emotion_colors = {\n",
    "            'angry': 'red',\n",
    "            'disgust': 'brown',\n",
    "            'fear': 'purple',\n",
    "            'happy': 'green',\n",
    "            'neutral': 'gray',\n",
    "            'sad': 'blue',\n",
    "            'surprise': 'orange'\n",
    "        }\n",
    "        \n",
    "        # Audio recording parameters\n",
    "        self.format = pyaudio.paFloat32\n",
    "        self.channels = 1\n",
    "        self.rate = 16000  # sample rate\n",
    "        self.chunk = 512  # smaller chunks for faster response\n",
    "        self.record_seconds = 1.5  # shorter window for quicker response\n",
    "        \n",
    "        # Sensitivity parameters\n",
    "        self.sensitivity = 1.2  # Amplification factor (higher = more sensitive)\n",
    "        self.threshold = 0.01  # Lower threshold for sound detection (lower = more sensitive)\n",
    "        self.smoothing_factor = 0.3  # Lower value means less smoothing (0-1)\n",
    "        \n",
    "        # Recording state\n",
    "        self.is_recording = False\n",
    "        self.audio_queue = queue.Queue()\n",
    "        self.emotion_history = []\n",
    "        self.current_emotion = \"neutral\"\n",
    "        self.current_probabilities = {emotion: 0.0 for emotion in self.emotions}\n",
    "        self.previous_probabilities = {emotion: 0.0 for emotion in self.emotions}\n",
    "        \n",
    "        # Initialize PyAudio\n",
    "        self.audio = pyaudio.PyAudio()\n",
    "        \n",
    "        # Create or load the model\n",
    "        if model_path and os.path.exists(model_path):\n",
    "            self._load_model(model_path, scaler_path)\n",
    "        else:\n",
    "            self.model = MLPClassifier(\n",
    "                hidden_layer_sizes=(256, 128, 64),\n",
    "                activation='relu',\n",
    "                solver='adam',\n",
    "                alpha=0.0001,\n",
    "                batch_size=256,\n",
    "                learning_rate='adaptive',\n",
    "                max_iter=300,\n",
    "                early_stopping=True,\n",
    "                verbose=True\n",
    "            )\n",
    "            self.scaler = StandardScaler()\n",
    "            print(\"New model created. You need to train it before use.\")\n",
    "    \n",
    "    def set_sensitivity(self, value):\n",
    "        \"\"\"Set the sensitivity amplification factor\"\"\"\n",
    "        self.sensitivity = value\n",
    "        print(f\"Sensitivity set to: {value}\")\n",
    "    \n",
    "    def set_threshold(self, value):\n",
    "        \"\"\"Set the audio detection threshold\"\"\"\n",
    "        self.threshold = value\n",
    "        print(f\"Threshold set to: {value}\")\n",
    "    \n",
    "    def set_smoothing(self, value):\n",
    "        \"\"\"Set the smoothing factor for emotion transitions\"\"\"\n",
    "        self.smoothing_factor = value\n",
    "        print(f\"Smoothing set to: {value}\")\n",
    "        \n",
    "    def extract_features(self, audio_data, sample_rate=16000, n_mfcc=13, n_mels=40, n_fft=1024, hop_length=256):\n",
    "        \"\"\"\n",
    "        Extract acoustic features from audio data with increased sensitivity\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        audio_data : np.ndarray\n",
    "            Audio data\n",
    "        sample_rate : int\n",
    "            Sample rate of the audio\n",
    "        n_mfcc : int\n",
    "            Number of MFCCs to extract\n",
    "        n_mels : int\n",
    "            Number of Mel bands\n",
    "        n_fft : int\n",
    "            FFT window size (smaller for better time resolution)\n",
    "        hop_length : int\n",
    "            Number of samples between frames (smaller for more features)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Feature vector\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Amplify the signal to increase sensitivity\n",
    "            audio_data = audio_data * self.sensitivity\n",
    "            \n",
    "            # Check if audio level is above threshold\n",
    "            if np.abs(audio_data).mean() < self.threshold:\n",
    "                return None\n",
    "            \n",
    "            # Trim silent parts with higher top_db (less aggressive trimming)\n",
    "            audio_data, _ = librosa.effects.trim(audio_data, top_db=15)\n",
    "            \n",
    "            # Extract features\n",
    "            # MFCCs with more coefficients for better detail\n",
    "            mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=n_mfcc+2, n_fft=n_fft, hop_length=hop_length)\n",
    "            mfccs_mean = np.mean(mfccs.T, axis=0)\n",
    "            mfccs_std = np.std(mfccs.T, axis=0)\n",
    "            mfccs_delta = librosa.feature.delta(mfccs)  # Add delta features for better sensitivity\n",
    "            mfccs_delta_mean = np.mean(mfccs_delta.T, axis=0)\n",
    "            \n",
    "            # Mel spectrogram with more bands\n",
    "            mel = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate, n_mels=n_mels+10, n_fft=n_fft, hop_length=hop_length)\n",
    "            mel_mean = np.mean(librosa.power_to_db(mel).T, axis=0)\n",
    "            mel_std = np.std(librosa.power_to_db(mel).T, axis=0)\n",
    "            \n",
    "            # Root Mean Square Energy with smaller frame size\n",
    "            rmse = librosa.feature.rms(y=audio_data, hop_length=hop_length)[0]\n",
    "            rmse_mean = np.mean(rmse)\n",
    "            rmse_std = np.std(rmse)\n",
    "            \n",
    "            # Zero Crossing Rate\n",
    "            zcr = librosa.feature.zero_crossing_rate(audio_data, hop_length=hop_length)[0]\n",
    "            zcr_mean = np.mean(zcr)\n",
    "            zcr_std = np.std(zcr)\n",
    "            \n",
    "            # Spectral centroid\n",
    "            centroid = librosa.feature.spectral_centroid(y=audio_data, sr=sample_rate, n_fft=n_fft, hop_length=hop_length)[0]\n",
    "            centroid_mean = np.mean(centroid)\n",
    "            centroid_std = np.std(centroid)\n",
    "            \n",
    "            # Spectral contrast with more bands\n",
    "            contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sample_rate, n_fft=n_fft, hop_length=hop_length)\n",
    "            contrast_mean = np.mean(contrast.T, axis=0)\n",
    "            contrast_std = np.std(contrast.T, axis=0)\n",
    "            \n",
    "            # Chroma features with more pitch classes\n",
    "            chroma = librosa.feature.chroma_stft(y=audio_data, sr=sample_rate, n_fft=n_fft, hop_length=hop_length)\n",
    "            chroma_mean = np.mean(chroma.T, axis=0)\n",
    "            chroma_std = np.std(chroma.T, axis=0)\n",
    "            \n",
    "            # Spectral roll-off (additional feature)\n",
    "            rolloff = librosa.feature.spectral_rolloff(y=audio_data, sr=sample_rate, n_fft=n_fft, hop_length=hop_length)[0]\n",
    "            rolloff_mean = np.mean(rolloff)\n",
    "            rolloff_std = np.std(rolloff)\n",
    "            \n",
    "            # Concatenate all features\n",
    "            features = np.concatenate([\n",
    "                mfccs_mean, mfccs_std, mfccs_delta_mean,\n",
    "                mel_mean, mel_std,\n",
    "                [rmse_mean, rmse_std],\n",
    "                [zcr_mean, zcr_std],\n",
    "                [centroid_mean, centroid_std],\n",
    "                contrast_mean, contrast_std,\n",
    "                chroma_mean, chroma_std,\n",
    "                [rolloff_mean, rolloff_std]\n",
    "            ])\n",
    "            \n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting features: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def save_model(self, model_path, scaler_path):\n",
    "        \"\"\"\n",
    "        Save the trained model and scaler\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_path : str\n",
    "            Path to save the model\n",
    "        scaler_path : str\n",
    "            Path to save the scaler\n",
    "        \"\"\"\n",
    "        joblib.dump(self.model, model_path)\n",
    "        \n",
    "        if self.scaler:\n",
    "            joblib.dump(self.scaler, scaler_path)\n",
    "        \n",
    "        print(f\"Model saved to {model_path} and scaler saved to {scaler_path}\")\n",
    "    \n",
    "    def _load_model(self, model_path, scaler_path=None):\n",
    "        \"\"\"\n",
    "        Load a trained model and scaler\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_path : str\n",
    "            Path to the saved model\n",
    "        scaler_path : str, optional\n",
    "            Path to the saved scaler (default: None)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model = joblib.load(model_path)\n",
    "            print(f\"Model loaded from {model_path}\")\n",
    "            \n",
    "            if scaler_path and os.path.exists(scaler_path):\n",
    "                self.scaler = joblib.load(scaler_path)\n",
    "                print(f\"Scaler loaded from {scaler_path}\")\n",
    "            else:\n",
    "                self.scaler = StandardScaler()\n",
    "                print(\"No scaler found. Using a new StandardScaler.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {str(e)}\")\n",
    "            self.model = MLPClassifier(\n",
    "                hidden_layer_sizes=(256, 128, 64),\n",
    "                activation='relu',\n",
    "                solver='adam'\n",
    "            )\n",
    "            self.scaler = StandardScaler()\n",
    "    \n",
    "    def predict_emotion(self, audio_data):\n",
    "        \"\"\"\n",
    "        Predict emotion from audio data with smoothing\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        audio_data : np.ndarray\n",
    "            Audio data\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (predicted_emotion, probabilities_dict)\n",
    "        \"\"\"\n",
    "        features = self.extract_features(audio_data)\n",
    "        \n",
    "        if features is None:\n",
    "            # If no features were extracted (silent audio)\n",
    "            # Return previous emotion with decreased confidence\n",
    "            decayed_probs = {\n",
    "                emotion: prob * 0.8 for emotion, prob in self.current_probabilities.items()\n",
    "            }\n",
    "            # Find emotion with highest probability\n",
    "            max_emotion = max(decayed_probs, key=decayed_probs.get)\n",
    "            return max_emotion, decayed_probs\n",
    "        \n",
    "        try:\n",
    "            # Scale features\n",
    "            features = self.scaler.transform(features.reshape(1, -1))\n",
    "            \n",
    "            # Predict\n",
    "            prediction = self.model.predict(features)[0]\n",
    "            raw_probabilities = self.model.predict_proba(features)[0]\n",
    "            \n",
    "            # Create raw probabilities dictionary\n",
    "            raw_probs = {emotion: 0.0 for emotion in self.emotions}\n",
    "            for emotion, prob in zip(self.model.classes_, raw_probabilities):\n",
    "                if emotion in raw_probs:\n",
    "                    raw_probs[emotion] = prob\n",
    "            \n",
    "            # Apply sensitivity amplification to the probabilities\n",
    "            # This increases the contrast between probabilities\n",
    "            amplified_probs = {}\n",
    "            for emotion, prob in raw_probs.items():\n",
    "                # Non-linear amplification that preserves the 0-1 range\n",
    "                amplified_probs[emotion] = 1 - (1 - prob) ** self.sensitivity\n",
    "            \n",
    "            # Apply smoothing between current and previous emotions\n",
    "            smoothed_probs = {}\n",
    "            for emotion in self.emotions:\n",
    "                prev_prob = self.current_probabilities.get(emotion, 0.0)\n",
    "                new_prob = amplified_probs.get(emotion, 0.0)\n",
    "                # Apply smoothing factor (lower = less smoothing, more responsive)\n",
    "                smoothed_probs[emotion] = (self.smoothing_factor * prev_prob) + ((1 - self.smoothing_factor) * new_prob)\n",
    "            \n",
    "            # Find emotion with highest probability\n",
    "            max_emotion = max(smoothed_probs, key=smoothed_probs.get)\n",
    "            \n",
    "            return max_emotion, smoothed_probs\n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {str(e)}\")\n",
    "            return \"neutral\", {emotion: 0.0 for emotion in self.emotions}\n",
    "    \n",
    "    def audio_callback(self, in_data, frame_count, time_info, status):\n",
    "        \"\"\"\n",
    "        Callback function for PyAudio\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        in_data : bytes\n",
    "            Input audio data\n",
    "        frame_count : int\n",
    "            Number of frames\n",
    "        time_info : dict\n",
    "            Timing information\n",
    "        status : int\n",
    "            Status flag\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (in_data, pyaudio.paContinue)\n",
    "        \"\"\"\n",
    "        audio_data = np.frombuffer(in_data, dtype=np.float32)\n",
    "        self.audio_queue.put(audio_data)\n",
    "        return (in_data, pyaudio.paContinue)\n",
    "    \n",
    "    def process_audio(self):\n",
    "        \"\"\"\n",
    "        Process audio from the queue and predict emotions\n",
    "        \"\"\"\n",
    "        buffer = np.array([])\n",
    "        buffer_duration = 0  # in seconds\n",
    "        \n",
    "        while self.is_recording:\n",
    "            if not self.audio_queue.empty():\n",
    "                audio_chunk = self.audio_queue.get()\n",
    "                \n",
    "                # Add chunk to buffer\n",
    "                buffer = np.append(buffer, audio_chunk)\n",
    "                chunk_duration = len(audio_chunk) / self.rate\n",
    "                buffer_duration += chunk_duration\n",
    "                \n",
    "                # If buffer is long enough, process it\n",
    "                if buffer_duration >= self.record_seconds:\n",
    "                    # Predict emotion\n",
    "                    emotion, probabilities = self.predict_emotion(buffer)\n",
    "                    \n",
    "                    # Update current emotion and probabilities\n",
    "                    self.previous_probabilities = self.current_probabilities.copy()\n",
    "                    self.current_emotion = emotion\n",
    "                    self.current_probabilities = probabilities\n",
    "                    self.emotion_history.append((emotion, probabilities))\n",
    "                    \n",
    "                    print(f\"Detected emotion: {emotion} - {probabilities[emotion]:.2f}\")\n",
    "                    \n",
    "                    # Reset buffer (with 75% overlap for smoother transitions)\n",
    "                    overlap_samples = int(self.rate * self.record_seconds * 0.75)\n",
    "                    if len(buffer) > overlap_samples:\n",
    "                        buffer = buffer[-overlap_samples:]\n",
    "                        buffer_duration = overlap_samples / self.rate\n",
    "                    else:\n",
    "                        buffer = np.array([])\n",
    "                        buffer_duration = 0\n",
    "            \n",
    "            time.sleep(0.01)  # Small delay to prevent CPU overuse\n",
    "    \n",
    "    def start_recording(self):\n",
    "        \"\"\"\n",
    "        Start recording audio from microphone\n",
    "        \"\"\"\n",
    "        if self.is_recording:\n",
    "            print(\"Already recording\")\n",
    "            return\n",
    "        \n",
    "        self.is_recording = True\n",
    "        \n",
    "        # Start stream\n",
    "        self.stream = self.audio.open(\n",
    "            format=self.format,\n",
    "            channels=self.channels,\n",
    "            rate=self.rate,\n",
    "            input=True,\n",
    "            frames_per_buffer=self.chunk,\n",
    "            stream_callback=self.audio_callback\n",
    "        )\n",
    "        \n",
    "        # Start processing thread\n",
    "        self.process_thread = threading.Thread(target=self.process_audio)\n",
    "        self.process_thread.daemon = True\n",
    "        self.process_thread.start()\n",
    "        \n",
    "        print(\"Recording started\")\n",
    "    \n",
    "    def stop_recording(self):\n",
    "        \"\"\"\n",
    "        Stop recording audio\n",
    "        \"\"\"\n",
    "        if not self.is_recording:\n",
    "            print(\"Not recording\")\n",
    "            return\n",
    "        \n",
    "        self.is_recording = False\n",
    "        \n",
    "        # Stop stream\n",
    "        self.stream.stop_stream()\n",
    "        self.stream.close()\n",
    "        \n",
    "        # Clear queue\n",
    "        while not self.audio_queue.empty():\n",
    "            self.audio_queue.get()\n",
    "        \n",
    "        print(\"Recording stopped\")\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Clean up resources\n",
    "        \"\"\"\n",
    "        if self.is_recording:\n",
    "            self.stop_recording()\n",
    "        \n",
    "        self.audio.terminate()\n",
    "        print(\"Resources released\")\n",
    "\n",
    "\n",
    "class EmotionRecognitionApp:\n",
    "    def __init__(self, root, model_path=None, scaler_path=None):\n",
    "        \"\"\"\n",
    "        GUI application for real-time emotion recognition\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        root : tk.Tk\n",
    "            Root window\n",
    "        model_path : str, optional\n",
    "            Path to a saved model file (default: None)\n",
    "        scaler_path : str, optional\n",
    "            Path to a saved scaler file (default: None)\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.root.title(\"Real-time Speech Emotion Recognition\")\n",
    "        self.root.geometry(\"800x700\")  # Increased height for controls\n",
    "        \n",
    "        # Initialize recognizer\n",
    "        self.recognizer = RealtimeSpeechEmotionRecognizer(model_path, scaler_path)\n",
    "        \n",
    "        # Create GUI\n",
    "        self.create_widgets()\n",
    "        \n",
    "        # Animation update interval (ms)\n",
    "        self.update_interval = 50  # Faster updates for more responsiveness\n",
    "        \n",
    "        # Set up animation\n",
    "        self.ani = animation.FuncAnimation(\n",
    "            self.fig, self.update_plot, interval=self.update_interval)\n",
    "        \n",
    "        # When window is closed\n",
    "        self.root.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n",
    "    \n",
    "    def create_widgets(self):\n",
    "        \"\"\"\n",
    "        Create GUI widgets\n",
    "        \"\"\"\n",
    "        # Main frame\n",
    "        main_frame = ttk.Frame(self.root, padding=\"10\")\n",
    "        main_frame.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Control frame\n",
    "        control_frame = ttk.Frame(main_frame, padding=\"5\")\n",
    "        control_frame.pack(fill=tk.X)\n",
    "        \n",
    "        # Start/Stop button\n",
    "        self.recording_state = tk.BooleanVar(value=False)\n",
    "        self.btn_record = ttk.Button(control_frame, text=\"Start Recording\", command=self.toggle_recording)\n",
    "        self.btn_record.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # Status label\n",
    "        self.lbl_status = ttk.Label(control_frame, text=\"Not recording\")\n",
    "        self.lbl_status.pack(side=tk.LEFT, padx=10)\n",
    "        \n",
    "        # Current emotion label\n",
    "        self.lbl_emotion = ttk.Label(control_frame, text=\"Emotion: --\", font=(\"Arial\", 14))\n",
    "        self.lbl_emotion.pack(side=tk.RIGHT, padx=10)\n",
    "        \n",
    "        # Sensitivity controls frame\n",
    "        sensitivity_frame = ttk.LabelFrame(main_frame, text=\"Sensitivity Controls\", padding=\"10\")\n",
    "        sensitivity_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        # Sensitivity slider\n",
    "        ttk.Label(sensitivity_frame, text=\"Sensitivity:\").grid(row=0, column=0, sticky=tk.W, padx=5)\n",
    "        self.sensitivity_slider = Scale(sensitivity_frame, from_=1.0, to=3.0, resolution=0.1, \n",
    "                                  orient=HORIZONTAL, length=300, command=self.update_sensitivity)\n",
    "        self.sensitivity_slider.set(1.2)\n",
    "        self.sensitivity_slider.grid(row=0, column=1, padx=5, pady=5)\n",
    "        \n",
    "        # Threshold slider\n",
    "        ttk.Label(sensitivity_frame, text=\"Detection Threshold:\").grid(row=1, column=0, sticky=tk.W, padx=5)\n",
    "        self.threshold_slider = Scale(sensitivity_frame, from_=0.001, to=0.05, resolution=0.001, \n",
    "                                orient=HORIZONTAL, length=300, command=self.update_threshold)\n",
    "        self.threshold_slider.set(0.01)\n",
    "        self.threshold_slider.grid(row=1, column=1, padx=5, pady=5)\n",
    "        \n",
    "        # Smoothing slider\n",
    "        ttk.Label(sensitivity_frame, text=\"Smoothing:\").grid(row=2, column=0, sticky=tk.W, padx=5)\n",
    "        self.smoothing_slider = Scale(sensitivity_frame, from_=0.0, to=0.9, resolution=0.05, \n",
    "                                orient=HORIZONTAL, length=300, command=self.update_smoothing)\n",
    "        self.smoothing_slider.set(0.3)\n",
    "        self.smoothing_slider.grid(row=2, column=1, padx=5, pady=5)\n",
    "        \n",
    "        # Matplotlib figure\n",
    "        self.fig = Figure(figsize=(8, 6), dpi=100)\n",
    "        \n",
    "        # Create two subplots\n",
    "        self.ax1 = self.fig.add_subplot(211)  # Audio waveform\n",
    "        self.ax2 = self.fig.add_subplot(212)  # Emotion probabilities\n",
    "        \n",
    "        # Create canvas\n",
    "        self.canvas = FigureCanvasTkAgg(self.fig, master=main_frame)\n",
    "        self.canvas.draw()\n",
    "        self.canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Initialize plots\n",
    "        self.waveform_line, = self.ax1.plot([], [], 'b-', linewidth=0.5)\n",
    "        self.ax1.set_title('Audio Waveform')\n",
    "        self.ax1.set_xlabel('Time (s)')\n",
    "        self.ax1.set_ylabel('Amplitude')\n",
    "        self.ax1.set_ylim(-1, 1)\n",
    "        self.ax1.set_xlim(0, self.recognizer.record_seconds)\n",
    "        \n",
    "        self.ax2.set_title('Emotion Probabilities')\n",
    "        self.ax2.set_xlabel('Emotion')\n",
    "        self.ax2.set_ylabel('Probability')\n",
    "        self.ax2.set_ylim(0, 1)\n",
    "        \n",
    "        self.bars = self.ax2.bar(self.recognizer.emotions, \n",
    "                                 [0] * len(self.recognizer.emotions),\n",
    "                                 color=[self.recognizer.emotion_colors.get(e, 'blue') for e in self.recognizer.emotions])\n",
    "        \n",
    "        self.fig.tight_layout()\n",
    "    \n",
    "    def update_sensitivity(self, value):\n",
    "        \"\"\"\n",
    "        Update sensitivity parameter\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        value : str\n",
    "            New sensitivity value\n",
    "        \"\"\"\n",
    "        self.recognizer.set_sensitivity(float(value))\n",
    "    \n",
    "    def update_threshold(self, value):\n",
    "        \"\"\"\n",
    "        Update threshold parameter\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        value : str\n",
    "            New threshold value\n",
    "        \"\"\"\n",
    "        self.recognizer.set_threshold(float(value))\n",
    "    \n",
    "    def update_smoothing(self, value):\n",
    "        \"\"\"\n",
    "        Update smoothing parameter\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        value : str\n",
    "            New smoothing value\n",
    "        \"\"\"\n",
    "        self.recognizer.set_smoothing(float(value))\n",
    "    \n",
    "    def toggle_recording(self):\n",
    "        \"\"\"\n",
    "        Toggle recording state\n",
    "        \"\"\"\n",
    "        if self.recording_state.get():\n",
    "            # Stop recording\n",
    "            self.recognizer.stop_recording()\n",
    "            self.recording_state.set(False)\n",
    "            self.btn_record.config(text=\"Start Recording\")\n",
    "            self.lbl_status.config(text=\"Not recording\")\n",
    "        else:\n",
    "            # Start recording\n",
    "            self.recognizer.start_recording()\n",
    "            self.recording_state.set(True)\n",
    "            self.btn_record.config(text=\"Stop Recording\")\n",
    "            self.lbl_status.config(text=\"Recording...\")\n",
    "    \n",
    "    def update_plot(self, frame):\n",
    "        \"\"\"\n",
    "        Update plot with current audio and emotion data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        frame : int\n",
    "            Animation frame number\n",
    "        \"\"\"\n",
    "        # Get the latest data from the queue for waveform display\n",
    "        audio_data = []\n",
    "        queue_copy = list(self.recognizer.audio_queue.queue)\n",
    "        \n",
    "        if queue_copy:\n",
    "            audio_data = np.concatenate(queue_copy)\n",
    "            \n",
    "            # Ensure we only show the latest data that fits our window\n",
    "            max_samples = int(self.recognizer.rate * self.recognizer.record_seconds)\n",
    "            if len(audio_data) > max_samples:\n",
    "                audio_data = audio_data[-max_samples:]\n",
    "            \n",
    "            # Update time axis\n",
    "            time_axis = np.linspace(0, len(audio_data) / self.recognizer.rate, len(audio_data))\n",
    "            self.ax1.set_xlim(0, time_axis[-1] if len(time_axis) > 0 else self.recognizer.record_seconds)\n",
    "            \n",
    "            # Update waveform plot\n",
    "            self.waveform_line.set_data(time_axis, audio_data)\n",
    "        \n",
    "        # Update emotion probabilities\n",
    "        for i, bar in enumerate(self.bars):\n",
    "            emotion = self.recognizer.emotions[i]\n",
    "            probability = self.recognizer.current_probabilities.get(emotion, 0)\n",
    "            bar.set_height(probability)\n",
    "            bar.set_color(self.recognizer.emotion_colors.get(emotion, 'blue'))\n",
    "        \n",
    "        # Update current emotion label with confidence\n",
    "        top_emotion = self.recognizer.current_emotion\n",
    "        top_probability = self.recognizer.current_probabilities.get(top_emotion, 0)\n",
    "        self.lbl_emotion.config(\n",
    "            text=f\"Emotion: {top_emotion.upper()} ({top_probability:.2f})\",\n",
    "            foreground=self.recognizer.emotion_colors.get(top_emotion, 'black')\n",
    "        )\n",
    "        \n",
    "        # Return the artists that were modified\n",
    "        return [self.waveform_line] + list(self.bars) + [self.lbl_emotion]\n",
    "    \n",
    "    def on_closing(self):\n",
    "        \"\"\"\n",
    "        Handle window closing event\n",
    "        \"\"\"\n",
    "        if self.recording_state.get():\n",
    "            self.recognizer.stop_recording()\n",
    "        \n",
    "        self.recognizer.close()\n",
    "        self.root.destroy()\n",
    "\n",
    "\n",
    "def download_pretrained_model():\n",
    "    \"\"\"\n",
    "    Function to download or create a pre-trained model\n",
    "    \n",
    "    This is a placeholder - in a real implementation, you would:\n",
    "    1. Check if a model exists locally\n",
    "    2. If not, download from a repository or create a dummy model\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (model_path, scaler_path)\n",
    "    \"\"\"\n",
    "    # For demonstration, we'll just create paths to where models would be stored\n",
    "    model_dir = os.path.join(os.path.expanduser(\"~\"), \".ser_models\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    model_path = os.path.join(model_dir, \"ser_model.pkl\")\n",
    "    scaler_path = os.path.join(model_dir, \"ser_scaler.pkl\")\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"Pre-trained model not found. In a real application, you would download it here.\")\n",
    "        print(\"For this demo, you'll need to train your own model or provide a pre-trained one.\")\n",
    "    \n",
    "    return model_path, scaler_path\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the application\n",
    "    \"\"\"\n",
    "    # Get model paths\n",
    "    model_path, scaler_path = download_pretrained_model()\n",
    "    \n",
    "    # Check if model exists\n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"No model found. You'll need to provide a pre-trained model.\")\n",
    "        print(\"For demonstration purposes, a dummy model would be created here.\")\n",
    "        # In a real app, you would train a model or provide instructions to download one\n",
    "    \n",
    "    # Create GUI\n",
    "    root = tk.Tk()\n",
    "    app = EmotionRecognitionApp(root, model_path, scaler_path)\n",
    "    root.mainloop()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4b40f6-f2bf-44ac-806e-51c3e5062843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
